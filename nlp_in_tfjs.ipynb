{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nlp_in_tfjs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "df89eb2137565137bb966753c531a753948024faef863ef170a73eb2b8e00875"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('tensorflow_env': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVNVV-MSM69Z"
      },
      "source": [
        "## **LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTwy9t-hM-ua",
        "outputId": "90c7a572-ab9a-4cc4-be6e-3e09922b2395"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "df = pd.read_csv('/content/dataset_UG-01-01.csv')\n",
        "df.shape"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2026, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cofl6pB8lHFo",
        "outputId": "2112170e-327c-48cd-bc51-d4420edc1d15"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_english</th>\n",
              "      <th>sentimen_textblob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>New DKI DKI SATPOL LOGE MO MUST</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Manggarai Water Gate Status Standby 2</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Transjakarta Corridor Setop Operations</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Anies must be the hands of optimal flood hooks</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jakut Social Sudin Ready to 500 Food Packages</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    clean_english sentimen_textblob\n",
              "0                 New DKI DKI SATPOL LOGE MO MUST          positive\n",
              "1           Manggarai Water Gate Status Standby 2           neutral\n",
              "2          Transjakarta Corridor Setop Operations           neutral\n",
              "3  Anies must be the hands of optimal flood hooks           neutral\n",
              "4   Jakut Social Sudin Ready to 500 Food Packages          positive"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "enc = LabelEncoder()\n",
        "df['sentimen_textblob'] = enc.fit_transform(df['sentimen_textblob'])"
      ],
      "metadata": {
        "id": "HQPHdpvY_ObT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentimen_textblob'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibGGY4m-_dKR",
        "outputId": "da763777-a8ae-4f4e-f0ea-77f94be69cd5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# sc = MinMaxScaler()\n",
        "# df['sentimen_textblob'] = sc.fit_transform(df['sentimen_textblob'].values)"
      ],
      "metadata": {
        "id": "_BFzFLCc-iJ4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDue0G_drWAl"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T7jl8UVrXo2"
      },
      "source": [
        "# convert to lowercase\n",
        "df['clean_english'] = df['clean_english'].str.lower()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOy-ylB1rfd4",
        "outputId": "7cb8380c-a5d4-4ea1-da9f-7fa60ed60a65"
      },
      "source": [
        "# remove stopwords\n",
        "\n",
        "#from nltk.corpus import stopwords #comment jika Error dan gunakan 2 sintaks dibawah\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Srwk40Vqcazy",
        "outputId": "b745b56b-ef7f-49e7-b25e-5e95fd336e42"
      },
      "source": [
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "df['clean_english'] = df['clean_english'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_english</th>\n",
              "      <th>sentimen_textblob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>new dki dki satpol loge mo must</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>manggarai water gate status standby 2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>transjakarta corridor setop operations</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>anies must hands optimal flood hooks</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jakut social sudin ready 500 food packages</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                clean_english  sentimen_textblob\n",
              "0             new dki dki satpol loge mo must                  2\n",
              "1       manggarai water gate status standby 2                  1\n",
              "2      transjakarta corridor setop operations                  1\n",
              "3        anies must hands optimal flood hooks                  1\n",
              "4  jakut social sudin ready 500 food packages                  2"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-lnuvqQrtUz"
      },
      "source": [
        "Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMjLCJclrsYj",
        "outputId": "2c06f6f8-ba8d-4f88-ed4d-312bc8eb1a27"
      },
      "source": [
        "vocab_size = 2026\n",
        "oov_tok = \"<OOV>\"\n",
        "filt = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ' #remove symbols\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok, filters = filt)\n",
        "tokenizer.fit_on_texts(df['clean_english'].values)\n",
        "\n",
        "word2index = tokenizer.word_index\n",
        "print(len(word2index))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whWt0LYxvO-C"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('word2index.json', 'w') as fp:\n",
        "    json.dump(word2index, fp)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ugC89x8r4La",
        "outputId": "b23e020d-7fc4-4f60-ffc1-6b8760b8989b"
      },
      "source": [
        "max_length =  max(len(values.split()) for i, values in enumerate(df['clean_english']))\n",
        "max_length"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkoanBtWr-Fq",
        "outputId": "28d0c803-8af0-44ac-86d1-99a59230a4ad"
      },
      "source": [
        "trunc_type='post'\n",
        "\n",
        "all_seq = tokenizer.texts_to_sequences(df['clean_english'].values)\n",
        "all_padded = pad_sequences(all_seq, maxlen = max_length, padding = trunc_type)\n",
        "all_padded.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2026, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSevKaW8rE2Y",
        "outputId": "44ede2d2-b793-4766-ca06-19d68fb5ad17"
      },
      "source": [
        "# split train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = all_padded\n",
        "#y = pd.get_dummies(df['label'].values)\n",
        "y = df['sentimen_textblob']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#kalimat = df['sentence'].values\n",
        "#y = df['label'].values\n",
        "\n",
        "#kalimat_latih, kalimat_test, y_latih, y_test = train_test_split(kalimat, y, \n",
        "#                                                                test_size=0.2, random_state=1000)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1620, 16) (1620,)\n",
            "(406, 16) (406,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train, 3)\n",
        "y_test = to_categorical(y_test, 3)"
      ],
      "metadata": {
        "id": "bjkhCVwFW41A"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUwJnNOZNp5K",
        "outputId": "852c6d73-c591-4331-ec6d-be8c20e8711a"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim = vocab_size, output_dim=16),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    # tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(3, activation='softmax'),\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 16)          32416     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                20736     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               8320      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,859\n",
            "Trainable params: 61,859\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbI-o4_TNvbH",
        "outputId": "b907ece6-f1af-4dc0-e1c8-13f20e01f883"
      },
      "source": [
        "num_epochs = 30\n",
        "history = model.fit(X_train, y_train, epochs=num_epochs, verbose=2, validation_data=(X_test, y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "51/51 - 7s - loss: 0.8756 - accuracy: 0.6932 - val_loss: 0.7297 - val_accuracy: 0.7217 - 7s/epoch - 130ms/step\n",
            "Epoch 2/30\n",
            "51/51 - 0s - loss: 0.6528 - accuracy: 0.7340 - val_loss: 0.5225 - val_accuracy: 0.7956 - 422ms/epoch - 8ms/step\n",
            "Epoch 3/30\n",
            "51/51 - 0s - loss: 0.3472 - accuracy: 0.8704 - val_loss: 0.4542 - val_accuracy: 0.8424 - 456ms/epoch - 9ms/step\n",
            "Epoch 4/30\n",
            "51/51 - 0s - loss: 0.2054 - accuracy: 0.9142 - val_loss: 0.4506 - val_accuracy: 0.8719 - 446ms/epoch - 9ms/step\n",
            "Epoch 5/30\n",
            "51/51 - 0s - loss: 0.1578 - accuracy: 0.9198 - val_loss: 0.4756 - val_accuracy: 0.8793 - 442ms/epoch - 9ms/step\n",
            "Epoch 6/30\n",
            "51/51 - 0s - loss: 0.1251 - accuracy: 0.9247 - val_loss: 0.4736 - val_accuracy: 0.8867 - 437ms/epoch - 9ms/step\n",
            "Epoch 7/30\n",
            "51/51 - 0s - loss: 0.1052 - accuracy: 0.9438 - val_loss: 0.5639 - val_accuracy: 0.8621 - 420ms/epoch - 8ms/step\n",
            "Epoch 8/30\n",
            "51/51 - 0s - loss: 0.0877 - accuracy: 0.9698 - val_loss: 0.5732 - val_accuracy: 0.8719 - 423ms/epoch - 8ms/step\n",
            "Epoch 9/30\n",
            "51/51 - 0s - loss: 0.0731 - accuracy: 0.9759 - val_loss: 0.7296 - val_accuracy: 0.8596 - 439ms/epoch - 9ms/step\n",
            "Epoch 10/30\n",
            "51/51 - 0s - loss: 0.0552 - accuracy: 0.9840 - val_loss: 0.8222 - val_accuracy: 0.8424 - 466ms/epoch - 9ms/step\n",
            "Epoch 11/30\n",
            "51/51 - 0s - loss: 0.0429 - accuracy: 0.9870 - val_loss: 0.8580 - val_accuracy: 0.8448 - 426ms/epoch - 8ms/step\n",
            "Epoch 12/30\n",
            "51/51 - 0s - loss: 0.0306 - accuracy: 0.9938 - val_loss: 0.8757 - val_accuracy: 0.8867 - 450ms/epoch - 9ms/step\n",
            "Epoch 13/30\n",
            "51/51 - 0s - loss: 0.0339 - accuracy: 0.9914 - val_loss: 0.7408 - val_accuracy: 0.8645 - 444ms/epoch - 9ms/step\n",
            "Epoch 14/30\n",
            "51/51 - 0s - loss: 0.0216 - accuracy: 0.9944 - val_loss: 1.0499 - val_accuracy: 0.8325 - 435ms/epoch - 9ms/step\n",
            "Epoch 15/30\n",
            "51/51 - 0s - loss: 0.0136 - accuracy: 0.9975 - val_loss: 1.1457 - val_accuracy: 0.8350 - 442ms/epoch - 9ms/step\n",
            "Epoch 16/30\n",
            "51/51 - 0s - loss: 0.0214 - accuracy: 0.9944 - val_loss: 0.9934 - val_accuracy: 0.8473 - 430ms/epoch - 8ms/step\n",
            "Epoch 17/30\n",
            "51/51 - 0s - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.9461 - val_accuracy: 0.8818 - 451ms/epoch - 9ms/step\n",
            "Epoch 18/30\n",
            "51/51 - 0s - loss: 0.0124 - accuracy: 0.9975 - val_loss: 1.2650 - val_accuracy: 0.8374 - 417ms/epoch - 8ms/step\n",
            "Epoch 19/30\n",
            "51/51 - 0s - loss: 0.0129 - accuracy: 0.9969 - val_loss: 1.0648 - val_accuracy: 0.8448 - 448ms/epoch - 9ms/step\n",
            "Epoch 20/30\n",
            "51/51 - 0s - loss: 0.0143 - accuracy: 0.9969 - val_loss: 0.9984 - val_accuracy: 0.8399 - 430ms/epoch - 8ms/step\n",
            "Epoch 21/30\n",
            "51/51 - 0s - loss: 0.0154 - accuracy: 0.9969 - val_loss: 1.2399 - val_accuracy: 0.8276 - 426ms/epoch - 8ms/step\n",
            "Epoch 22/30\n",
            "51/51 - 0s - loss: 0.0260 - accuracy: 0.9944 - val_loss: 0.8625 - val_accuracy: 0.8325 - 437ms/epoch - 9ms/step\n",
            "Epoch 23/30\n",
            "51/51 - 0s - loss: 0.0272 - accuracy: 0.9932 - val_loss: 0.7369 - val_accuracy: 0.8571 - 413ms/epoch - 8ms/step\n",
            "Epoch 24/30\n",
            "51/51 - 0s - loss: 0.0256 - accuracy: 0.9932 - val_loss: 0.7838 - val_accuracy: 0.8424 - 448ms/epoch - 9ms/step\n",
            "Epoch 25/30\n",
            "51/51 - 0s - loss: 0.0184 - accuracy: 0.9957 - val_loss: 0.8280 - val_accuracy: 0.8744 - 449ms/epoch - 9ms/step\n",
            "Epoch 26/30\n",
            "51/51 - 0s - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.9169 - val_accuracy: 0.8621 - 436ms/epoch - 9ms/step\n",
            "Epoch 27/30\n",
            "51/51 - 0s - loss: 0.0128 - accuracy: 0.9981 - val_loss: 0.7856 - val_accuracy: 0.8522 - 463ms/epoch - 9ms/step\n",
            "Epoch 28/30\n",
            "51/51 - 0s - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.9349 - val_accuracy: 0.8522 - 436ms/epoch - 9ms/step\n",
            "Epoch 29/30\n",
            "51/51 - 0s - loss: 0.0104 - accuracy: 0.9975 - val_loss: 0.7286 - val_accuracy: 0.8818 - 433ms/epoch - 8ms/step\n",
            "Epoch 30/30\n",
            "51/51 - 0s - loss: 0.0093 - accuracy: 0.9981 - val_loss: 0.7036 - val_accuracy: 0.8695 - 445ms/epoch - 9ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdJLOUkoMtF6"
      },
      "source": [
        "#def toSequence(sentence):\n",
        "#  pad = []\n",
        "#  for stc in sentence.split():\n",
        "#    if stc.lower() in word2index.keys(): \n",
        "#      pad.append(word2index[stc.lower()])\n",
        "#    else: \n",
        "#      continue\n",
        "#  return pad\n",
        "\n",
        "#pad = toSequence('affordable price and nice dessert')\n",
        "#pad = [269, 353, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0 ,0,0,0,0]\n",
        "#len(pad)\n",
        "#model.predict([pad])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-fp_i2PGt4J"
      },
      "source": [
        "Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsTz_o4zGnWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e701d278-8e6b-4b82-f2e8-49b28109987a"
      },
      "source": [
        "!pip install tensorflowjs"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-3.12.0-py3-none-any.whl (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 71 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (2.7.0)\n",
            "Requirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (12.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.10.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.37.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.7.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.22.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.13.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.42.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.7.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.19.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.1.1)\n",
            "Installing collected packages: tensorflowjs\n",
            "Successfully installed tensorflowjs-3.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3brVf1AGzcw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b234f5e-1a64-4f48-e7f4-e02200cb29f3"
      },
      "source": [
        "saved_model_path = '/content/mymodel/'\n",
        "tf.saved_model.save(model, saved_model_path)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/mymodel/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/mymodel/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0L5e5ocG418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e378df30-6770-4707-fe7d-5628759527dd"
      },
      "source": [
        "!tensorflowjs_converter \\\n",
        "  --input_format=tf_saved_model \\\n",
        "  /content/mymodel/ \\\n",
        "  /content/modeltfjs"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-13 13:15:51.118107: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Writing weight file /content/modeltfjs/model.json...\n"
          ]
        }
      ]
    }
  ]
}